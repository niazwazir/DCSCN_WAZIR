{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70b3b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c02adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_LIST =['augmented_data/train_org/',\n",
    "           'augmented_data/train_sr/HR',\n",
    "           'augmented_data/train_sr/LRBICUBIC',\n",
    "           'augmented_data/train_sr/LRX2',\n",
    "           'save_model']\n",
    "               \n",
    "for dir in DIR_LIST:\n",
    "    os.makedirs(dir, exist_ok=True)  \n",
    "    \n",
    "DATA_DIR = ['data/bsd200', 'data/yang91']\n",
    "OUTPUT_DIR = 'augmented_data/train_org/'\n",
    "\n",
    "BICUBIC_DIR = 'augmented_data/train_sr/LRBICUBIC'\n",
    "LRX2_DIR = 'augmented_data/train_sr/LRX2'\n",
    "HR_DIR = 'augmented_data/train_sr/HR'\n",
    "\n",
    "TEST_DIR = 'data/set5'\n",
    "TEST_OUTPUT_DIR = 'test/'\n",
    "MODEL_PATH = 'save_model/WAZIR_V2_e100_lr0.0001.pt'\n",
    "\n",
    "SCALE_FACTOR = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef88ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAugmentedImg(file_list):\n",
    "    \n",
    "    img_org = cv2.imread(file_list[0])\n",
    "    # original image\n",
    "    \n",
    "    img_0 = img_org\n",
    "    path_0 = file_list[2] + \"img_\" + str(file_list[1]) + \"_0\" + \".png\"\n",
    "    cv2.imwrite(path_0, img_0)\n",
    "    # original image\n",
    "    \n",
    "    img_1 = np.flipud(img_org)\n",
    "    path_1 = file_list[2] + \"img_\" + str(file_list[1]) + \"_1\"  + \".png\"\n",
    "    cv2.imwrite(path_1, img_1)\n",
    "    # flip vertical\n",
    "    \n",
    "    img_2 = np.fliplr(img_org)\n",
    "    path_2 = file_list[2] + \"img_\" + str(file_list[1]) + \"_2\"  + \".png\"\n",
    "    cv2.imwrite(path_2, img_2)\n",
    "    # flip horizontal\n",
    "    \n",
    "    img_3 = np.fliplr(np.flipud(img_org))\n",
    "    path_3 = file_list[2] + \"img_\" + str(file_list[1]) + \"_3\"  + \".png\"\n",
    "    cv2.imwrite(path_3, img_3)\n",
    "    # flip vertical - horizontal\n",
    "    \n",
    "    img_4 = np.rot90(img_org)\n",
    "    path_4 = file_list[2] + \"img_\" + str(file_list[1]) + \"_4\"  + \".png\"\n",
    "    cv2.imwrite(path_4, img_4)\n",
    "    # rotation 90 degree(CCW)\n",
    "    \n",
    "    img_5 = np.rot90(img_org, -1)\n",
    "    path_5 = file_list[2] + \"img_\" + str(file_list[1]) + \"_5\"  + \".png\"\n",
    "    cv2.imwrite(path_5, img_5)\n",
    "    # rotation 270 degree(CCW)\n",
    "    \n",
    "    img_6 = np.flipud(np.rot90(img_org))\n",
    "    path_6 = file_list[2] + \"img_\" + str(file_list[1]) + \"_6\"  + \".png\"\n",
    "    cv2.imwrite(path_6, img_6)\n",
    "    # rotation 90 degree(CCW) - flip vertical\n",
    "\n",
    "    img_7 = np.flipud(np.rot90(img_org, -1))\n",
    "    path_7 = file_list[2] + \"img_\" + str(file_list[1]) + \"_7\"  + \".png\"\n",
    "    cv2.imwrite(path_7, img_7)\n",
    "    # rotation 270 degree(CCW) - flip vertical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bdf80f",
   "metadata": {},
   "source": [
    "# 데이터 - 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "600c4df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 original image loaded\n",
      "augmented image genarated\n"
     ]
    }
   ],
   "source": [
    "file_list = []\n",
    "file_num = 0\n",
    "\n",
    "for i in DATA_DIR:\n",
    "    for j, file in enumerate(sorted(glob(i + '/*'))):\n",
    "        file_list.append([file, file_num, OUTPUT_DIR])\n",
    "        file_num += 1\n",
    "        \n",
    "print(len(file_list), \"original image loaded\")\n",
    "# load original image\n",
    "\n",
    "\n",
    "for i in range(len(file_list)):\n",
    "    generateAugmentedImg(file_list[i])\n",
    "    # generate Augmented images\n",
    "    \n",
    "print(\"augmented image genarated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e71e6a",
   "metadata": {},
   "source": [
    "# 데이터 - HR, LR, BI 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5742557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvtBGR2Y(image):\n",
    "\n",
    "    R = image[:,:,2]\n",
    "    G = image[:,:,1]\n",
    "    B = image[:,:,0]\n",
    "    \n",
    "    # Y' = 16  +  65.481  * R' + 128.553  * G' +  24.966  * B'\n",
    "    Y = (16*255.0/256.0)  + (65.481/256.0*R +  128.553/256.0*G +  24.966/256.0*B)\n",
    "    # convert cv2 BGR image to Y(CbCr) image\n",
    "        \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec1fffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizeImage(image, scale):\n",
    "    \n",
    "    width, height = image.shape[1], image.shape[0]\n",
    "    new_width = int(width * scale)\n",
    "    new_height = int(height * scale)\n",
    "    \n",
    "    image = Image.fromarray(image.reshape(height, width))\n",
    "    image = image.resize([new_width, new_height], Image.BICUBIC)\n",
    "    image = np.asarray(image)\n",
    "    image = image.reshape(new_height, new_width)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbf9ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitPatches(image, window_size, stride):\n",
    " \n",
    "    size = image.itemsize\n",
    "    # image item size\n",
    "    \n",
    "    shape = (1 + (image.shape[0] - window_size) // stride,\n",
    "            1 + (image.shape[1] - window_size) // stride,\n",
    "             window_size,\n",
    "             window_size)\n",
    "    strides = size * np.array([image.shape[1] * stride, stride, image.shape[1], 1])\n",
    "             \n",
    "    return np.lib.stride_tricks.as_strided(image, shape, strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06f3b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImgPatches(image, window_size, stride):\n",
    "\n",
    "    patches = splitPatches(image, window_size, stride)\n",
    "    # split images into patches(sliding window)\n",
    "    \n",
    "    patches = patches.reshape(patches.shape[0] * patches.shape[1], 1, patches.shape[2], patches.shape[3])\n",
    "    # reshape patches\n",
    "    \n",
    "    height, width = image.shape[0], image.shape[1]\n",
    "    \n",
    "    extra_patches = []\n",
    "    if (image.shape[0] - window_size) % stride != 0:\n",
    "        for x in range(0, image.shape[1] - window_size, stride):\n",
    "            extra_patches.append(image[image.shape[0] - window_size - 1:image.shape[0] - 1, x:x + window_size:])\n",
    "\n",
    "    if (width - window_size) % stride != 0:\n",
    "        for y in range(0, image.shape[0] - window_size, stride):\n",
    "            extra_patches.append(image[y: y + window_size, image.shape[1] - window_size - 1:image.shape[1] - 1])\n",
    "\n",
    "    if len(extra_patches) > 0:\n",
    "        org_size = patches.shape[0]\n",
    "        patches = np.resize(patches, [org_size + len(extra_patches), patches.shape[1], patches.shape[2], patches.shape[3]])\n",
    "        for i in range(len(extra_patches)):\n",
    "            extra_patches[i] = extra_patches[i].reshape([1,extra_patches[i].shape[0], extra_patches[i].shape[1]])\n",
    "            patches[org_size + i] = extra_patches[i]\n",
    "    \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bc46799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPatchList(train_list, HR_DIR, LRX2_DIR, BICUBIC_DIR):\n",
    "    \n",
    "    patch_size = 32\n",
    "    patch_stride = 16\n",
    "    # lr patch size\n",
    "    \n",
    "    patch_hr_size = patch_size * SCALE_FACTOR\n",
    "    patch_hr_stride = patch_stride * SCALE_FACTOR\n",
    "    # hr, bi patch size\n",
    "\n",
    "    hr_list = []\n",
    "    lr_list = []\n",
    "    bi_list = []\n",
    "\n",
    "    for image_path in train_list:\n",
    "        image = cv2.imread(image_path)\n",
    "        # load image(BGR)\n",
    "        \n",
    "        width, height = (image.shape[1]//SCALE_FACTOR) * SCALE_FACTOR, (image.shape[0]//SCALE_FACTOR)*SCALE_FACTOR\n",
    "        hr_img = image[:height, :width, 0:3]\n",
    "        # Adjust the image size to a multiple of SCALE_FACTOR.\n",
    "        \n",
    "        hr_y = cvtBGR2Y(hr_img)\n",
    "        lr_y = resizeImage(hr_y, 1/SCALE_FACTOR)\n",
    "        bi_y = resizeImage(lr_y, SCALE_FACTOR)\n",
    "        \n",
    "        #lr_y = cv2.resize(hr_y, dsize=(0, 0), fx=(1/SCALE_FACTOR), fy=(1/SCALE_FACTOR), interpolation=cv2.INTER_CUBIC)\n",
    "        #bi_y = cv2.resize(lr_y, dsize=(0, 0), fx=SCALE_FACTOR, fy=SCALE_FACTOR, interpolation=cv2.INTER_CUBIC)\n",
    "        # convert image to Y image, resize into lr & bi\n",
    "        # cv2 resize function result is not same with PIL resize!!!\n",
    "                \n",
    "        hr_y_patches = getImgPatches(hr_y, patch_hr_size, patch_hr_stride)\n",
    "        lr_y_patches = getImgPatches(lr_y, patch_size, patch_stride)\n",
    "        bi_y_patches = getImgPatches(bi_y, patch_hr_size, patch_hr_stride)\n",
    "        # split hr, lr, bi into patches\n",
    "              \n",
    "        hr_list.append(hr_y_patches)\n",
    "        lr_list.append(lr_y_patches)\n",
    "        bi_list.append(bi_y_patches)\n",
    "        # append hr. lr, bi Y patches into list\n",
    "        \n",
    "    hr_array = np.concatenate(hr_list)\n",
    "    lr_array = np.concatenate(lr_list)\n",
    "    bi_array = np.concatenate(bi_list)\n",
    "    # concatenate list into array  (2328*[num_patches,1,patch_size,patch_size]) --> (2328*num_patches,1,patch_size,patch_size)\n",
    "\n",
    "    return hr_array, lr_array, bi_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d15e5612",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_list = []\n",
    "\n",
    "for i in (sorted(glob(OUTPUT_DIR + '/*'))):\n",
    "        train_list.append(i)\n",
    "\n",
    "HR_ARRAY, LR_ARRAY, BI_ARRAY = getPatchList(train_list, HR_DIR, LRX2_DIR, BICUBIC_DIR)\n",
    "# split train images --> hr, lr, bi patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb69d73",
   "metadata": {},
   "source": [
    "# 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19b43681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hr_array, lr_array, bi_array, transform=None):\n",
    "        self.hr_array = hr_array\n",
    "        self.lr_array = lr_array\n",
    "        self.bi_array = bi_array\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.hr_array)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        hr = self.hr_array[index]\n",
    "        lr = self.lr_array[index]\n",
    "        bi = self.bi_array[index]\n",
    "        \n",
    "        return hr, lr, bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bc355b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(HR_ARRAY, LR_ARRAY, BI_ARRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280bc83c",
   "metadata": {},
   "source": [
    "# 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8058af91",
   "metadata": {},
   "source": [
    "![nn](./figure1_v2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f91dd928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAZIR(\n",
      "  (drop): Dropout(p=0.8, inplace=False)\n",
      "  (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "  (conv1): Conv2d(1, 196, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv2): Conv2d(196, 166, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv3): Conv2d(166, 148, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv4): Conv2d(148, 133, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv5): Conv2d(133, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv6): Conv2d(120, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv7): Conv2d(108, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv8): Conv2d(97, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv9): Conv2d(86, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv10): Conv2d(76, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv11): Conv2d(66, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv12): Conv2d(57, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv13): Conv2d(66, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (A1): Conv2d(1376, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B1): Conv2d(1376, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (UPsample): Conv2d(96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (pixelshuffle): PixelShuffle(upscale_factor=2)\n",
      "  (R): Conv2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class WAZIR(nn.Module):\n",
    "    \n",
    "    def __init__(self, kernel_size = 3, n_channels = 64):\n",
    "        super(WAZIR, self).__init__()\n",
    "                \n",
    "        # common layer\n",
    "        self.drop = nn.Dropout(p=0.8)\n",
    "        #self.prelu = nn.PReLU()\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        # feature extraction layer\n",
    "        self.conv1 = nn.Conv2d(1, 196, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv2 = nn.Conv2d(196, 166, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv3 = nn.Conv2d(166, 148, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv4 = nn.Conv2d(148, 133, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv5 = nn.Conv2d(133, 120, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv6 = nn.Conv2d(120, 108, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv7 = nn.Conv2d(108, 97, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv8 = nn.Conv2d(97, 86, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv9 = nn.Conv2d(86, 76, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv10 = nn.Conv2d(76, 66, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv11 = nn.Conv2d(66, 57, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv12 = nn.Conv2d(57, 66, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        self.conv13 = nn.Conv2d(66, 57, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        #self.conv14 = nn.Conv2d(120, 108, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        #self.conv15 = nn.Conv2d(108, 97, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        #self.conv16 = nn.Conv2d(97, 86, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        #self.conv17 = nn.Conv2d(86, 76, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        #self.conv18 = nn.Conv2d(76, 66, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        #self.conv19 = nn.Conv2d(66, 57, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        #self.conv20 = nn.Conv2d(57, 48, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        \n",
    "        ## he initialization\n",
    "        nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight)\n",
    "        nn.init.kaiming_normal_(self.conv4.weight)\n",
    "        nn.init.kaiming_normal_(self.conv5.weight)\n",
    "        nn.init.kaiming_normal_(self.conv6.weight)\n",
    "        nn.init.kaiming_normal_(self.conv7.weight)\n",
    "        nn.init.kaiming_normal_(self.conv8.weight)\n",
    "        nn.init.kaiming_normal_(self.conv9.weight)\n",
    "        nn.init.kaiming_normal_(self.conv10.weight)\n",
    "        nn.init.kaiming_normal_(self.conv11.weight)\n",
    "        nn.init.kaiming_normal_(self.conv12.weight)\n",
    "        nn.init.kaiming_normal_(self.conv13.weight)\n",
    "        #nn.init.kaiming_normal_(self.conv14.weight)\n",
    "        #nn.init.kaiming_normal_(self.conv15.weight)\n",
    "        #nn.init.kaiming_normal_(self.conv16.weight)\n",
    "        #nn.init.kaiming_normal_(self.conv17.weight)\n",
    "        #nn.init.kaiming_normal_(self.conv18.weight)\n",
    "        #nn.init.kaiming_normal_(self.conv19.weight)\n",
    "        #nn.init.kaiming_normal_(self.conv20.weight)\n",
    "\n",
    "        \n",
    "        # reconstruction layer\n",
    "        self.A1 = nn.Conv2d(1376, 64, 1, stride = 1, bias = True)\n",
    "        self.B1 = nn.Conv2d(1376, 32, 1, stride = 1, bias = True)\n",
    "        self.B2 = nn.Conv2d(32, 32, kernel_size, stride = 1, padding=1,padding_mode=\"replicate\",bias=True)\n",
    "        \n",
    "        ## he initialization\n",
    "        nn.init.kaiming_normal_(self.A1.weight)\n",
    "        nn.init.kaiming_normal_(self.B1.weight)\n",
    "        nn.init.kaiming_normal_(self.B2.weight)\n",
    "        \n",
    "        # feature extraction layer\n",
    "        self.UPsample = nn.Conv2d(96, 2*2*96, kernel_size, stride = 1, padding =1, padding_mode = \"replicate\", bias = True)\n",
    "        self.pixelshuffle = nn.PixelShuffle(2)\n",
    "        \n",
    "        self.R = nn.Conv2d(96, 1, kernel_size, stride = 1, padding = 1, padding_mode = \"replicate\", bias = True)\n",
    "        \n",
    "        ## he initialization\n",
    "        nn.init.kaiming_normal_(self.UPsample.weight)\n",
    "        nn.init.kaiming_normal_(self.R.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "                \n",
    "        # feature extraction network\n",
    "        skip1 = self.drop(self.leakyrelu(self.conv1(x)))\n",
    "        skip2 = self.drop(self.leakyrelu(self.conv2(skip1)))\n",
    "        skip3 = self.drop(self.leakyrelu(self.conv3(skip2)))\n",
    "        skip4 = self.drop(self.leakyrelu(self.conv4(skip3)))\n",
    "        skip5 = self.drop(self.leakyrelu(self.conv5(skip4)))\n",
    "        skip6 = self.drop(self.leakyrelu(self.conv6(skip5)))\n",
    "        skip7 = self.drop(self.leakyrelu(self.conv7(skip6)))\n",
    "        skip8 = self.drop(self.leakyrelu(self.conv8(skip7)))\n",
    "        skip9 = self.drop(self.leakyrelu(self.conv9(skip8)))\n",
    "        skip10 = self.drop(self.leakyrelu(self.conv10(skip9)))\n",
    "        skip11 = self.drop(self.leakyrelu(self.conv11(skip10)))\n",
    "        skip12 = self.drop(self.leakyrelu(self.conv12(skip11)))\n",
    "        skip13 = self.drop(self.leakyrelu(self.conv13(skip12)))\n",
    "        #skip14 = self.drop(self.leakyrelu(self.conv14(skip13)))\n",
    "        #skip15 = self.drop(self.leakyrelu(self.conv15(skip14)))\n",
    "        #skip16 = self.drop(self.leakyrelu(self.conv16(skip15)))\n",
    "        #skip17 = self.drop(self.leakyrelu(self.conv17(skip16)))\n",
    "        #skip18 = self.drop(self.leakyrelu(self.conv18(skip17)))\n",
    "        #skip19 = self.drop(self.leakyrelu(self.conv19(skip18)))\n",
    "        #skip20 = self.drop(self.leakyrelu(self.conv20(skip19)))\n",
    "        \n",
    "        # reconstruction network\n",
    "        recon_input = torch.cat([skip1, skip2, skip3, skip4, skip5, skip6, skip7, skip8, skip9, skip10, skip11, skip12,skip13], dim = 1)\n",
    "        \n",
    "        A1_out = self.drop(self.leakyrelu(self.A1(recon_input)))\n",
    "        \n",
    "        B1_out = self.drop(self.leakyrelu(self.B1(recon_input)))\n",
    "        B2_out = self.drop(self.leakyrelu(self.B2(B1_out)))\n",
    "        \n",
    "        recon_output = torch.cat([A1_out, B2_out], dim = 1)\n",
    "        \n",
    "        # up-sampling network\n",
    "        UPsample_out = self.drop(self.pixelshuffle(self.UPsample(recon_output)))\n",
    "        R_out = self.R(UPsample_out)\n",
    "        \n",
    "        return R_out\n",
    "model = WAZIR()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d1f997",
   "metadata": {},
   "source": [
    "# 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ab0f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "num_epochs = 4\n",
    "BATCH_SIZE  = 20\n",
    "model_path = 'save_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a467c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        drop_last=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69dec1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "rows = 1\n",
    "cols = 3\n",
    "\n",
    "for hr, lr, bi in data_loader:\n",
    "    ax1 = fig.add_subplot(rows, cols, 1)\n",
    "    ax1.imshow(hr[0][0], cmap='gray')\n",
    "    ax1.set_title('hr')\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    ax2 = fig.add_subplot(rows, cols, 2)\n",
    "    ax2.imshow(lr[0][0], cmap='gray')\n",
    "    ax2.set_title('lr')\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    ax3 = fig.add_subplot(rows, cols, 3)\n",
    "    ax3.imshow(bi[0][0], cmap='gray')\n",
    "    ax3.set_title('bi')\n",
    "    ax3.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "890756de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[232.4224, 232.4224, 232.4224,  ..., 229.0005, 229.0005, 229.0005],\n",
      "        [232.4224, 232.4224, 232.4224,  ..., 228.1451, 228.1451, 228.1451],\n",
      "        [232.4224, 232.4224, 232.4224,  ..., 228.1451, 228.1451, 228.1451],\n",
      "        ...,\n",
      "        [228.1451, 228.1451, 228.1451,  ..., 100.3783,  95.1385,  88.0997],\n",
      "        [228.1451, 228.1451, 228.1451,  ...,  90.7679,  90.5635,  89.0014],\n",
      "        [228.1451, 228.1451, 228.1451,  ...,  82.3202,  87.4436,  90.9168]],\n",
      "       dtype=torch.float64)\n",
      "tensor([[232.4012, 232.6317, 232.8710,  ..., 228.6811, 228.2615, 228.5630],\n",
      "        [232.3953, 232.6944, 232.9943,  ..., 228.7823, 228.3149, 228.1042],\n",
      "        [232.5037, 232.0092, 231.5407,  ..., 229.1756, 228.7800, 228.0836],\n",
      "        ...,\n",
      "        [228.1827, 227.7776, 227.3360,  ..., 126.6217,  99.8756,  86.3969],\n",
      "        [228.1890, 227.7123, 227.2287,  ..., 115.7896, 101.3014,  89.6323],\n",
      "        [228.2702, 227.7822, 227.2912,  ...,  95.3908,  86.5292,  89.0847]])\n",
      "tensor([[232.3819, 232.4392, 232.5604,  ..., 228.2981, 228.5236, 228.6323],\n",
      "        [232.3747, 232.4413, 232.5825,  ..., 228.2750, 228.4231, 228.4939],\n",
      "        [232.3593, 232.4460, 232.6294,  ..., 228.2259, 228.2102, 228.2009],\n",
      "        ...,\n",
      "        [228.2717, 228.1463, 227.8807,  ...,  95.7093,  90.7723,  89.7450],\n",
      "        [228.3660, 228.2365, 227.9625,  ...,  88.9273,  88.6081,  89.7026],\n",
      "        [228.1424, 228.0290, 227.7890,  ...,  84.9533,  88.1518,  90.9406]])\n"
     ]
    }
   ],
   "source": [
    "for hr, lr, bi in data_loader:\n",
    "    print(hr[0][0])\n",
    "    print(lr[0][0])\n",
    "    print(bi[0][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af2960",
   "metadata": {},
   "source": [
    "# 모델 로드 & 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a06d0458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAZIR(\n",
      "  (drop): Dropout(p=0.8, inplace=False)\n",
      "  (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "  (conv1): Conv2d(1, 196, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv2): Conv2d(196, 166, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv3): Conv2d(166, 148, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv4): Conv2d(148, 133, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv5): Conv2d(133, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv6): Conv2d(120, 108, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv7): Conv2d(108, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv8): Conv2d(97, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv9): Conv2d(86, 76, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv10): Conv2d(76, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv11): Conv2d(66, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv12): Conv2d(57, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (conv13): Conv2d(66, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (A1): Conv2d(1376, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B1): Conv2d(1376, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (B2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (UPsample): Conv2d(96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "  (pixelshuffle): PixelShuffle(upscale_factor=2)\n",
      "  (R): Conv2d(96, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      ")\n",
      "Epoch: 1, Loss: 1395499335680.0, LR: 0.0001\n",
      "Epoch 1 model saved.\n",
      "Epoch: 2, Loss: 554795073536.0, LR: 0.0001\n",
      "Epoch: 3, Loss: 311344300032.0, LR: 0.0001\n",
      "Epoch 3 model saved.\n",
      "Epoch: 4, Loss: 202576330752.0, LR: 0.0001\n",
      "EPOCH 4 last model saved.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = WAZIR().to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_func = nn.MSELoss().to(device)\n",
    "\n",
    "for epochs in range(num_epochs):  \n",
    "    \n",
    "    for HR, LR, BI in data_loader:\n",
    "                   \n",
    "        recon = model(LR.to(device).float())\n",
    "        recon += BI.to(device).float()\n",
    "        loss = loss_func(recon, HR.to(device).float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Epoch: {}, Loss: {}, LR: {}'.format(epochs+1, loss.item(), learning_rate))\n",
    "    ##############if epoch % 10 == 0:\n",
    "    if epochs % 2 == 0:\n",
    "        save_model_path = model_path + \"/WAZIR_V2_e{}_lr{}_loss{:4}.pt\".format(epochs, learning_rate, loss.item())\n",
    "        torch.save(model, save_model_path)\n",
    "        print(\"Epoch {} model saved.\".format(epochs+1))\n",
    "        # save model per 10 epochs\n",
    "\n",
    "save_model_path = model_path + \"/WAZIRN_V2_e{}_lr{}.pt\".format(epochs+1, learning_rate)\n",
    "torch.save(model, save_model_path)\n",
    "print(\"EPOCH {} last model saved.\".format(epochs+1))\n",
    "# save last model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e381e95",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93748540",
   "metadata": {},
   "source": [
    "평가 코드는 공정성을 위해 레퍼런스 코드 그대로 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37a9918",
   "metadata": {},
   "source": [
    "evaluation notebook 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7464b98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+\n",
      "|     Modules     | Parameters |\n",
      "+-----------------+------------+\n",
      "|   conv1.weight  |    1764    |\n",
      "|    conv1.bias   |    196     |\n",
      "|   conv2.weight  |   292824   |\n",
      "|    conv2.bias   |    166     |\n",
      "|   conv3.weight  |   221112   |\n",
      "|    conv3.bias   |    148     |\n",
      "|   conv4.weight  |   177156   |\n",
      "|    conv4.bias   |    133     |\n",
      "|   conv5.weight  |   143640   |\n",
      "|    conv5.bias   |    120     |\n",
      "|   conv6.weight  |   116640   |\n",
      "|    conv6.bias   |    108     |\n",
      "|   conv7.weight  |   94284    |\n",
      "|    conv7.bias   |     97     |\n",
      "|   conv8.weight  |   75078    |\n",
      "|    conv8.bias   |     86     |\n",
      "|   conv9.weight  |   58824    |\n",
      "|    conv9.bias   |     76     |\n",
      "|  conv10.weight  |   45144    |\n",
      "|   conv10.bias   |     66     |\n",
      "|  conv11.weight  |   33858    |\n",
      "|   conv11.bias   |     57     |\n",
      "|  conv12.weight  |   33858    |\n",
      "|   conv12.bias   |     66     |\n",
      "|  conv13.weight  |   33858    |\n",
      "|   conv13.bias   |     57     |\n",
      "|    A1.weight    |   88064    |\n",
      "|     A1.bias     |     64     |\n",
      "|    B1.weight    |   44032    |\n",
      "|     B1.bias     |     32     |\n",
      "|    B2.weight    |    9216    |\n",
      "|     B2.bias     |     32     |\n",
      "| UPsample.weight |   331776   |\n",
      "|  UPsample.bias  |    384     |\n",
      "|     R.weight    |    864     |\n",
      "|      R.bias     |     1      |\n",
      "+-----------------+------------+\n",
      "Total Trainable Params: 1803881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1803881"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29caa6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import imageio\n",
    "from glob import glob\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from skimage.metrics import structural_similarity\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "737a087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(OUTPUT_DIR,expected_totalaug=0,test=False):\n",
    "\n",
    "    split_list = []\n",
    "    for i in (sorted(glob(OUTPUT_DIR + '/*'))):\n",
    "        split_list.append(i)\n",
    "\n",
    "    if test:\n",
    "        pass\n",
    "    else:\n",
    "        if len(split_list) != expected_totalaug:\n",
    "            print(\"Not fittable the number of augmented images !!\")\n",
    "            exit()\n",
    "        else:\n",
    "            print(\"Next Level : Split images Using Window, {} pic\".format(len(split_list)))\n",
    "\n",
    "    return split_list\n",
    "\n",
    "def resize_image_by_pil(image, scale):\n",
    "    width, height = image.shape[1], image.shape[0]\n",
    "    new_width = int(width * scale)\n",
    "    new_height = int(height * scale)\n",
    "    method = Image.BICUBIC\n",
    "\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        image = Image.fromarray(image, \"RGB\")\n",
    "        image = image.resize([new_width, new_height], resample=method)\n",
    "        image = np.asarray(image)\n",
    "    elif len(image.shape) == 3 and image.shape[2] == 4:\n",
    "        # the image may has an alpha channel\n",
    "        image = Image.fromarray(image, \"RGB\")\n",
    "        image = image.resize([new_width, new_height], resample=method)\n",
    "        image = np.asarray(image)\n",
    "    else:\n",
    "        image = Image.fromarray(image.reshape(height, width))\n",
    "        image = image.resize([new_width, new_height], resample=method)\n",
    "        image = np.asarray(image)\n",
    "        image = image.reshape(new_height, new_width, 1)\n",
    "    return image\n",
    "\n",
    "\n",
    "def save_image(filename, image, print_console=True):\n",
    "    if len(image.shape) >= 3 and image.shape[0] == 1:\n",
    "        image = image.reshape(image.shape[1], image.shape[2])\n",
    "\n",
    "    directory = os.path.dirname(filename)\n",
    "    if directory != \"\" and not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    image = image.astype(np.uint8)\n",
    "    if len(image.shape) >= 3 and image.shape[0] == 3:\n",
    "        image = Image.fromarray(image, mode=\"RGB\")\n",
    "    else:\n",
    "        image = Image.fromarray(image)\n",
    "    imageio.imwrite(filename, image)\n",
    "\n",
    "    if print_console:\n",
    "        print(\"Saved [%s]\" % filename)\n",
    "\n",
    "\n",
    "def set_image_alignment(image, alignment):\n",
    "    alignment = int(alignment)\n",
    "    width, height = image.shape[1], image.shape[0]\n",
    "    width = (width // alignment) * alignment\n",
    "    height = (height // alignment) * alignment\n",
    "\n",
    "    if image.shape[1] != width or image.shape[0] != height:\n",
    "        image = image[:height, :width, :]\n",
    "\n",
    "    if len(image.shape) >= 3 and image.shape[2] >= 4:\n",
    "        image = image[:, :, 0:3]\n",
    "\n",
    "    return image\n",
    "\n",
    "def convert_rgb_to_y(image, jpeg_mode=False, max_value=255.0):\n",
    "    if len(image.shape) <= 2 or image.shape[2] == 1:\n",
    "        return image\n",
    "\n",
    "    if jpeg_mode:\n",
    "        xform = np.array([[0.299, 0.587, 0.114]])\n",
    "        y_image = image.dot(xform.T)\n",
    "    else:\n",
    "        xform = np.array([[65.481 / 256.0, 128.553 / 256.0, 24.966 / 256.0]])\n",
    "        y_image = image.dot(xform.T) + (16.0 * max_value / 256.0)\n",
    "\n",
    "    return y_image\n",
    "\n",
    "def convert_rgb_to_ycbcr(image):\n",
    "    if len(image.shape) < 2 or image.shape[2] == 1:\n",
    "        return image\n",
    "\n",
    "    xform = np.array(\n",
    "        [[65.738 / 256.0, 129.057 / 256.0, 25.064 / 256.0],\n",
    "         [- 37.945 / 256.0, - 74.494 / 256.0, 112.439 / 256.0],\n",
    "         [112.439 / 256.0, - 94.154 / 256.0, - 18.285 / 256.0]])\n",
    "\n",
    "    ycbcr_image = image.dot(xform.T)\n",
    "    ycbcr_image[:, :, 0] += 16.0\n",
    "    ycbcr_image[:, :, [1, 2]] += 128.0\n",
    "\n",
    "    return ycbcr_image\n",
    "\n",
    "\n",
    "def convert_y_and_cbcr_to_rgb(y_image, cbcr_image):\n",
    "    print(cbcr_image.shape)\n",
    "    if len(y_image.shape) <= 2:\n",
    "        y_image = y_image.reshape[y_image.shape[0], y_image.shape[1], 1]\n",
    "\n",
    "    if len(y_image.shape) == 3 and y_image.shape[2] == 3:\n",
    "        y_image = y_image[:, :, 0:1]\n",
    "\n",
    "    ycbcr_image = np.zeros([y_image.shape[0], y_image.shape[1], 3])\n",
    "    ycbcr_image[:, :, 0] = y_image[:, :, 0]\n",
    "    ycbcr_image[:, :, 1:3] = cbcr_image[:, :, 0:2]\n",
    "\n",
    "    return convert_ycbcr_to_rgb(ycbcr_image)\n",
    "\n",
    "def convert_ycbcr_to_rgb(ycbcr_image):\n",
    "    rgb_image = np.zeros([ycbcr_image.shape[0], ycbcr_image.shape[1], 3])  # type: np.ndarray\n",
    "\n",
    "    rgb_image[:, :, 0] = ycbcr_image[:, :, 0] - 16.0\n",
    "    rgb_image[:, :, [1, 2]] = ycbcr_image[:, :, [1, 2]] - 128.0\n",
    "    xform = np.array(\n",
    "        [[298.082 / 256.0, 0, 408.583 / 256.0],\n",
    "         [298.082 / 256.0, -100.291 / 256.0, -208.120 / 256.0],\n",
    "         [298.082 / 256.0, 516.412 / 256.0, 0]])\n",
    "    rgb_image = rgb_image.dot(xform.T)\n",
    "\n",
    "    return rgb_image\n",
    "def get_split_images(image, window_size, stride=None, enable_duplicate=True):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 1:\n",
    "        image = image.reshape(image.shape[0], image.shape[1])\n",
    "    #print(image.shape)\n",
    "    window_size = int(window_size)\n",
    "    size = image.itemsize\n",
    "    height, width = image.shape\n",
    "    if stride is None:\n",
    "        stride = window_size\n",
    "    else:\n",
    "        stride = int(stride)\n",
    "\n",
    "    if height < window_size or width < window_size:\n",
    "        return None\n",
    "\n",
    "    new_height = 1 + (height - window_size) // stride\n",
    "    new_width = 1 + (width - window_size) //  stride\n",
    "\n",
    "    shape = (new_height, new_width, window_size, window_size)\n",
    "    strides = size * np.array([width * stride, stride, width, 1])\n",
    "    windows = np.lib.stride_tricks.as_strided(image, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "    windows = windows.reshape(windows.shape[0] * windows.shape[1],1, windows.shape[2], windows.shape[3])\n",
    "\n",
    "    if enable_duplicate:\n",
    "        extra_windows = []\n",
    "        if (height - window_size) % stride != 0:\n",
    "            for x in range(0, width - window_size, stride):\n",
    "                extra_windows.append(image[height - window_size - 1:height - 1, x:x + window_size:])\n",
    "\n",
    "        if (width - window_size) % stride != 0:\n",
    "            for y in range(0, height - window_size, stride):\n",
    "                extra_windows.append(image[y: y + window_size, width - window_size - 1:width - 1])\n",
    "\n",
    "        if len(extra_windows) > 0:\n",
    "            org_size = windows.shape[0]\n",
    "            windows = np.resize(windows,\n",
    "                                [org_size + len(extra_windows), windows.shape[1], windows.shape[2], windows.shape[3]])\n",
    "            for i in range(len(extra_windows)):\n",
    "                extra_windows[i] = extra_windows[i].reshape([1,extra_windows[i].shape[0], extra_windows[i].shape[1]])\n",
    "                windows[org_size + i] = extra_windows[i]\n",
    "\n",
    "    return windows\n",
    "\n",
    "def GPU_AVAILABLE():\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print('Device:', device)  # 출력결과: cuda\n",
    "    print('Count of using GPUs:', torch.cuda.device_count())  # 출력결과: 2 (2, 3 두개 사용하므로)\n",
    "    print('Current cuda device:', torch.cuda.current_device())  # 출력결과: 2 (2, 3 중 앞의 GPU #2 의미)\n",
    "    return device\n",
    "\n",
    "def compute_psnr_and_ssim(image1, image2, border_size=0):\n",
    "    \"\"\"\n",
    "    Computes PSNR and SSIM index from 2 images.\n",
    "    We round it and clip to 0 - 255. Then shave 'scale' pixels from each border.\n",
    "    \"\"\"\n",
    "    if len(image1.shape) == 2:\n",
    "        image1 = image1.reshape(image1.shape[0], image1.shape[1], 1)\n",
    "    if len(image2.shape) == 2:\n",
    "        image2 = image2.reshape(image2.shape[0], image2.shape[1], 1)\n",
    "\n",
    "    if image1.shape[0] != image2.shape[0] or image1.shape[1] != image2.shape[1] or image1.shape[2] != image2.shape[2]:\n",
    "        return None\n",
    "\n",
    "    image1 = trim_image_as_file(image1)\n",
    "    image2 = trim_image_as_file(image2)\n",
    "\n",
    "    if border_size > 0:\n",
    "        image1 = image1[border_size:-border_size, border_size:-border_size, :]\n",
    "        image2 = image2[border_size:-border_size, border_size:-border_size, :]\n",
    "\n",
    "    psnr = peak_signal_noise_ratio(image1, image2, data_range=255)\n",
    "    ssim = structural_similarity(image1, image2, win_size=11, gaussian_weights=True, multichannel=True, K1=0.01, K2=0.03,\n",
    "                        sigma=1.5, data_range=255)\n",
    "    return psnr, ssim\n",
    "\n",
    "\n",
    "def trim_image_as_file(image):\n",
    "    image = np.rint(image)\n",
    "    image = np.clip(image, 0, 255)\n",
    "    if image.dtype != np.float32:\n",
    "        image = image.astype(np.float32)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33b56364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n"
     ]
    }
   ],
   "source": [
    "# Checking GPU Available\n",
    "\n",
    "# splited 된 그림을 보길 원하시면 batch_picture_save_flag 를 1 로 바꾸시면 됩니다.\n",
    "# 경로 : augmented_data/train_sr\n",
    "batch_picture_save_flag = 0\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)  # 출력결과: cuda\n",
    "print('Count of using GPUs:', torch.cuda.device_count())  # 출력결과: 2 (2, 3 두개 사용하므로)\n",
    "print('Current cuda device:', torch.cuda.current_device())  # 출력결과: 2 (2, 3 중 앞의 GPU #2 의미)\n",
    "\n",
    "# Configure Data Augmentation\n",
    "\n",
    "DATA_DIR = ['data/bsd200', 'data/yang91']\n",
    "OUTPUT_DIR = 'augmented_data/train_org/'\n",
    "\n",
    "# Split Parameters\n",
    "\n",
    "BICUBIC_DIR = 'augmented_data/train_sr/LRBICUBIC'\n",
    "LRX2_DIR = 'augmented_data/train_sr/LRX2'\n",
    "HR_DIR = 'augmented_data/train_sr/HR'\n",
    "\n",
    "lr_batch_size = 32\n",
    "scale = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f87dfe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_path,lr,bi):\n",
    "\n",
    "    device = GPU_AVAILABLE()\n",
    "\n",
    "    if len(lr.shape) == 3 and lr.shape[2] == 1:\n",
    "        lr_luma = lr.reshape(1,1,lr.shape[0], lr.shape[1])\n",
    "    if len(bi.shape) == 3 and bi.shape[2] == 1:\n",
    "        bi_luma = bi.reshape(1,1,bi.shape[0], bi.shape[1])\n",
    "\n",
    "    lr_luma = torch.FloatTensor(lr_luma).to(device)\n",
    "    bi_luma = torch.FloatTensor(bi_luma).to(device)\n",
    "    \n",
    "    load_model = torch.load(model_path).to(device)\n",
    "    load_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = load_model(lr_luma)\n",
    "        pred += bi_luma\n",
    "\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    pred = pred.reshape(pred.shape[2],pred.shape[3],1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbdc3a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(512, 512, 2)\n",
      "(512, 512, 2)\n",
      "luma_recon : 6.858801401191967 / 0.02795715443789959 luma_bicubic : 37.12439095115347 / 0.9520454406738281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_18204/3298102410.py:206: FutureWarning: `multichannel` is a deprecated argument name for `structural_similarity`. It will be removed in version 1.0.Please use `channel_axis` instead.\n",
      "  ssim = structural_similarity(image1, image2, win_size=11, gaussian_weights=True, multichannel=True, K1=0.01, K2=0.03,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(288, 288, 2)\n",
      "(288, 288, 2)\n",
      "luma_recon : 9.66448093705154 / 0.05842145159840584 luma_bicubic : 36.73963134778147 / 0.9719822406768799\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(256, 256, 2)\n",
      "(256, 256, 2)\n",
      "luma_recon : 7.331540814504864 / 0.07528065890073776 luma_bicubic : 27.485831267157707 / 0.9150046706199646\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(280, 280, 2)\n",
      "(280, 280, 2)\n",
      "luma_recon : 10.064846471161948 / 0.06277147680521011 luma_bicubic : 34.90877421068931 / 0.8621172308921814\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(344, 228, 2)\n",
      "(344, 228, 2)\n",
      "luma_recon : 7.314249153296137 / 0.049908727407455444 luma_bicubic : 32.1838099118808 / 0.9474995136260986\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(276, 276, 2)\n",
      "(276, 276, 2)\n",
      "luma_recon : 10.06081707753592 / 0.06257705390453339 luma_bicubic : 34.890131994263214 / 0.8619232177734375\n",
      "Device: cuda\n",
      "Count of using GPUs: 1\n",
      "Current cuda device: 0\n",
      "(362, 500, 2)\n",
      "(362, 500, 2)\n",
      "luma_recon : 9.513847749344112 / 0.05101035535335541 luma_bicubic : 30.45732337846438 / 0.8987409472465515\n",
      "avg_psnr / avg_ssim : 8.6869405148695 / 0.05541812548679965\n"
     ]
    }
   ],
   "source": [
    "# model load\n",
    "\n",
    "load_path = 'save_model/WAZIRN_V2_e4_lr0.0001.pt'\n",
    "\n",
    "# test image load\n",
    "\n",
    "test_dir = 'data/set5'\n",
    "test_list = load_img(test_dir,test=True)\n",
    "scale_factor = 2\n",
    "output_dir = 'test'\n",
    "\n",
    "AVG_PSNR = []\n",
    "AVG_SSIM = []\n",
    "for i in test_list:\n",
    "    file_name = os.path.basename(i)\n",
    "    file_name,ext = os.path.splitext(file_name)\n",
    "\n",
    "    img = imageio.imread(i)\n",
    "    lr_img = resize_image_by_pil(img,1/scale_factor)\n",
    "    bi_img = resize_image_by_pil(lr_img,scale_factor)\n",
    "\n",
    "    y_img = convert_rgb_to_y(img)\n",
    "    y_lr_img = resize_image_by_pil(y_img,1/scale_factor)\n",
    "    y_bi_img = resize_image_by_pil(y_lr_img,scale_factor)\n",
    "    ycbcr_bi_img = convert_rgb_to_ycbcr(bi_img)\n",
    "\n",
    "    recon = test(load_path,y_lr_img,y_bi_img)\n",
    "    recon_rgb = convert_y_and_cbcr_to_rgb(recon,ycbcr_bi_img[:,:,1:3])\n",
    "\n",
    "    bicubic_rgb = convert_y_and_cbcr_to_rgb(y_bi_img,ycbcr_bi_img[:,:,1:3])\n",
    "\n",
    "    luma_psnr,luma_ssim = compute_psnr_and_ssim(y_img, recon, 2+scale_factor)\n",
    "    luma_bipsnr, luma_bissim = compute_psnr_and_ssim(y_img, y_bi_img, 0)\n",
    "\n",
    "    print(\"luma_recon : {} / {} luma_bicubic : {} / {}\".format(luma_psnr, luma_ssim, luma_bipsnr, luma_bissim))\n",
    "\n",
    "    org_name = output_dir+file_name + '_org' + ext\n",
    "    recon_save_name = output_dir+file_name + '_recon' + ext\n",
    "    bi_save_name = output_dir+file_name + '_bi' + ext\n",
    "\n",
    "    recon_rgb = np.clip(recon_rgb,0,255)\n",
    "    bicubic_rgb = np.clip(bicubic_rgb,0,255)\n",
    "\n",
    "    AVG_PSNR.append(luma_psnr)\n",
    "    AVG_SSIM.append(luma_ssim)\n",
    "\n",
    "avg_psnr = sum(AVG_PSNR)/len(AVG_PSNR)\n",
    "avg_ssim = sum(AVG_SSIM)/len(AVG_SSIM)\n",
    "\n",
    "print(\"avg_psnr / avg_ssim : {} / {}\".format(avg_psnr,avg_ssim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce42c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_pytorch",
   "language": "python",
   "name": "cuda_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
